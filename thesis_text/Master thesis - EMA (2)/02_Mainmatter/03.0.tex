\chapter{Clustering}
\label{chap:clustering}
Clustering is a method of finding structural patterns in given data and sorting the data points into groups or subsets. In general, data points in one cluster are more similar to each other than to data points in other clusters. Clustering can be done in either supervised, semisupervised, or unsupervised fashion. One has to keep in mind that the definition of a cluster is in the eye of the scientist.

Cluster analysis is a collection of various algorithms and methods that can be used in said grouping process, with each being appropriate for different jobs. The basis of each model is different as well, using various measures such as distance \citep{murtagh2017algorithms, hsu2007hierarchical}. The basic overview can be split into connectivity models \citep{fischer2003clustering}, centroid models \citep{morissette2013k, sun2014parallel}, distribution models \citep{jimenez2019extensions, bocchieri2001subspace}, density models \citep{campello2020density}, and other. This thesis focuses mainly on hierarchical clustering, which falls under connectivity models and uses distance as its metric of choice. 

However, as the definition of what a cluster is is vague, new clustering algorithms are developed, old clustering algorithms are revisited and refined \citep{estivill2002so}.

Cluster analysis can be used in a variety of settings, be it image or text analysis, e-commerce, data mining \citep{agarwal2013data}, and biology. Clustering can stand on its own or be a part of a data analysis pipeline. 

\section{Hierarchical clustering}
\label{sec:hierarchicalclustering}
Hierarchical clustering, also known as connectivity-based clustering, is one of the possible approaches to classification problems. It is a tool that gives an idea of data structure, and allows for association between subclusters while also keeping a level of distinction. Hierarchical clustering imagines that two data points can be closer to each other than to other data points. Hierarchical clustering can be done in two ways: bottom up, also known as agglomerative, in which the displayed distance is of two clusters, and top down, also known as divisive, in which the distance displayed is between individual observations. Only the agglomerative approach will be discussed in this thesis. 

In the agglomerative approach, two closest elements are conjoined into a cluster, which is then joined into a larger cluster with the closest element. In this iterative way, a singular final cluster is made and a hierarchical overview of the data is formed. In this way, an agglomerative hierarchical algorithm displays the similarity between clusters and only check distances between data points.

Hierarchical clustering can be visually represented by dendrograms, as described in \ref{sec:dendrogram}.


"""
In hierarchical clustering, the merging points of clusters are called "nodes" or "linkage points".
Each merging point is represented by a node in the dendrogram, which is the graphical representation of the hierarchical clustering.
A dendrogram is a tree-like diagram that shows the hierarchical structure of the clusters at different levels of granularity. The leaves of the dendrogram represent individual data points and the branches represent clusters of data points, which are formed by merging smaller clusters.
The linkage point show the distance between the two clusters that are being merged, and the height of the linkage point in the dendrogram represents the distance or similarity between the merged clusters."""

\section{Distance}
\label{sec:distance}
Distance can never be a negative number, and scales from 0 for objects that are not different at all, to larger values for more different objects. The default distance metric is usually Euclidean, although in flow cytometry context, the usage of Mahalanobis distance instead could be a better idea \cite{fivser2012detection}.

Euclidean distance is a distance metric used between two points in spaces of higher dimension than 2. It can be found by using the Pythagorean theorem and solving for hypotenuse. 


 \[ x^2 + y^2= z^2 \]
 ADD d(p, q) = root((p1-q1)\^2 + (pn-qn)\^2))

 It can also be found using polar coordinates:

 ADD d(p,q) = root(r\^2 + s\^2 - 2rs cos(theta-phi)) 

Mahalanobis distance is useful for keeping both orientation and axes ratios while serving as an expansion factor for the best fitted ellipsoid, allowing for more efficient processing of elongated clusters \cite{zamir2005resolving}. It is a distance metric distances used for distance between a point P and distribution D. 

It follows this algorithm:

1. find centroid/center of mass

2. Estimate standard deviation of distances between points and center of mass

3. If distance(test point, center point) < 1 stdev -> point belongs to the set with a high pobability

4. Plug the above, which can also be written as 
\[(test_point - sample_mean)/standard_deviation\]
